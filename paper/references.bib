@article{davis_unanswerable_images,
  author  = {Ernest Davis},
  title   = {Unanswerable Questions About Images and Texts},
  journal = {Frontiers in Artificial Intelligence},
  volume  = {3},
  pages   = {51},
  year    = {2020},
  doi     = {10.3389/frai.2020.00051},
  url     = {https://doi.org/10.3389/frai.2020.00051}
}

@inproceedings{gurari_vizwiz_2018,
  author    = {Danna Gurari and Qing Li and Abigale J. Stangl and Anhong Guo and Chi Lin and Kristen Grauman and Jiebo Luo and Jeffrey P. Bigham},
  title     = {VizWiz Grand Challenge: Answering Visual Questions from Blind People},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2018},
  pages     = {3608--3617}
}

@inproceedings{rajpurkar_squad2_2018,
  author    = {Pranav Rajpurkar and Robin Jia and Percy Liang},
  title     = {Know What You Do Not Know: Unanswerable Questions for {SQuAD}},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL)},
  year      = {2018},
  pages     = {784--789}
}

@inproceedings{recasens_gazefollow_2015,
  author    = {Adri\`{a} Recasens and Aditya Khosla and Carl Vondrick and Antonio Torralba},
  title     = {Where Are They Looking?},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2015},
  pages     = {199--207}
}

% NOTE: This is a conference paper, so inproceedings is the cleanest BibTeX type.
@inproceedings{recasens_following_gaze_2017,
  author    = {Adri\`{a} Recasens and Carl Vondrick and Aditya Khosla and Antonio Torralba},
  title     = {Following Gaze Across Views},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  year      = {2017}
}

@inproceedings{jiang_peekaboo_2020,
  author    = {Ziyu Jiang and Buyu Liu and Samuel Schulter and Zhangyang Wang and Manmohan Chandraker},
  title     = {Peek-a-Boo: Occlusion Reasoning in Indoor Scenes with Plane Representations},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2020},
  pages     = {113--121}
}

@inproceedings{hudson_gqa_2019,
  author    = {Drew A. Hudson and Christopher D. Manning},
  title     = {GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2019}
}

@article{kosinski_tom_llms_2023,
  author  = {Michal Kosinski},
  title   = {Theory of Mind May Have Spontaneously Emerged in Large Language Models},
  journal = {arXiv preprint},
  volume  = {arXiv:2302.02083},
  year    = {2023},
  url     = {https://arxiv.org/abs/2302.02083}
}

@article{ullman_tom_perturbations,
  author  = {Tomer D. Ullman},
  title   = {Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks},
  journal = {arXiv preprint},
  volume  = {arXiv:2302.08399},
  year    = {2023},
  url     = {https://arxiv.org/abs/2302.08399}
}

@inproceedings{geifman_selectivenet_2019,
  author    = {Yonatan Geifman and Ran El{-}Yaniv},
  title     = {SelectiveNet: A Deep Neural Network with an Integrated Reject Option},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning (ICML)},
  year      = {2019},
  pages     = {2151--2159},
  publisher = {PMLR}
}

% -------------------------
% NEW: citations used in the rewritten paper (additions)
% -------------------------

@article{elyaniv_selective_2010,
  author  = {Ran El{-}Yaniv},
  title   = {On the Foundations of Noise-Free Selective Classification},
  journal = {Journal of Machine Learning Research},
  volume  = {11},
  number  = {May},
  pages   = {1605--1641},
  year    = {2010},
  url     = {https://jmlr.org/papers/v11/el-yaniv10a.html}
}

@article{varshney_selective_2022,
  author  = {Neeraj Varshney and Swaroop Mishra and Pallavi Vijayakumar and Chitta Baral},
  title   = {Investigating Selective Prediction Approaches Across NLP Tasks},
  journal = {Findings of the Association for Computational Linguistics},
  year    = {2022},
  url     = {https://aclanthology.org/2022.findings-acl.}
}

@article{fu_mme_2023,
  author  = {Chaoyou Fu and Peixian Chen and Yunhang Shen and Yulei Qin and Mengdan Zhang and Xu Lin and Jinrui Yang and Xiawu Zheng and Ke Li and Xing Sun and Yunsheng Wu and Rongrong Ji},
  title   = {MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models},
  journal = {arXiv preprint},
  volume  = {arXiv:2306.13394},
  year    = {2023},
  url     = {https://arxiv.org/abs/2306.13394}
}

@inproceedings{li_pope_2023,
  author    = {Yifan Li and others},
  title     = {Evaluating Object Hallucination in Large Vision-Language Models},
  booktitle = {Proceedings of EMNLP},
  year      = {2023},
  note      = {Also available as arXiv:2305.10355},
  url       = {https://arxiv.org/abs/2305.10355}
}

@article{gemini_1_5_technical_report_2024,
  author  = {Gemini Team},
  title   = {Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  journal = {arXiv preprint},
  volume  = {arXiv:2403.05530},
  year    = {2024},
  url     = {https://arxiv.org/abs/2403.05530}
}

@article{liu_llava_onevision_2024,
  author  = {Haotian Liu and others},
  title   = {LLaVA-OneVision: Easy Visual Task Transfer},
  journal = {arXiv preprint},
  volume  = {arXiv:2408.03326},
  year    = {2024},
  url     = {https://arxiv.org/abs/2408.03326}
}

@article{qwen2_vl_2024,
  author  = {Qwen Team},
  title   = {Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},
  journal = {arXiv preprint},
  volume  = {arXiv:2409.12191},
  year    = {2024},
  url     = {https://arxiv.org/abs/2409.12191}
}

@article{qwen3_vl_2025,
  author  = {Shuai Bai and others},
  title   = {Qwen3-VL Technical Report},
  journal = {arXiv preprint},
  volume  = {arXiv:2511.21631},
  year    = {2025},
  url     = {https://arxiv.org/abs/2511.21631}
}

@article{internvl3_2025,
  author  = {Jinguo Zhu and others},
  title   = {InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models},
  journal = {arXiv preprint},
  volume  = {arXiv:2504.10479},
  year    = {2025},
  url     = {https://arxiv.org/abs/2504.10479}
}

@article{gemma3_2025,
  author  = {Gemma Team},
  title   = {Gemma 3 Technical Report},
  journal = {arXiv preprint},
  volume  = {arXiv:2503.19786},
  year    = {2025},
  url     = {https://arxiv.org/abs/2503.19786}
}

% OpenAI GPT-4o reference (system card is the most citable public doc)
@misc{openai_gpt4o_system_card_2024,
  author       = {OpenAI},
  title        = {GPT-4o System Card},
  year         = {2024},
  howpublished = {\url{https://openai.com/index/gpt-4o-system-card/}},
  note         = {Accessed 2026-01-03}
}

% Llama 3.2 Vision model card (GitHub)
@misc{meta_llama32_vision_model_card_2024,
  author       = {Meta},
  title        = {Llama 3.2 Vision Model Card},
  year         = {2024},
  howpublished = {\url{https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md}},
  note         = {Accessed 2026-01-03}
}

% Anthropic Claude models overview (docs)
@misc{anthropic_claude_models_overview_2025,
  author       = {Anthropic},
  title        = {Claude Models Overview},
  year         = {2025},
  howpublished = {\url{https://platform.claude.com/docs/en/about-claude/models/overview}},
  note         = {Accessed 2026-01-03}
}

% Optional: include these if you cite them in Related Work (they were in my earlier draft).
@article{guo_unkvqa_2023,
  author  = {Yangyang Guo and Fangkai Jiao and Zhiqi Shen and Liqiang Nie and Mohan S. Kankanhalli},
  title   = {UNK-VQA: A Dataset and a Probe into the Abstention Ability of Multi-modal Large Models},
  journal = {arXiv preprint},
  volume  = {arXiv:2310.10942},
  year    = {2023},
  url     = {https://arxiv.org/abs/2310.10942}
}

@article{he_tubench_2024,
  author  = {Xingwei He and Qianru Zhang and A{-}Long Jin and Yuan Yuan and Siu{-}Ming Yiu},
  title   = {TUBench: Benchmarking Large Vision-Language Models on Trustworthiness with Unanswerable Questions},
  journal = {arXiv preprint},
  volume  = {arXiv:2410.04107},
  year    = {2024},
  url     = {https://arxiv.org/abs/2410.04107}
}
